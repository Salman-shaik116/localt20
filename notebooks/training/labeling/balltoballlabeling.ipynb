{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9566 - loss: 0.1276 - val_accuracy: 0.5788 - val_loss: 2.1243 - learning_rate: 5.0000e-05\n",
      "Epoch 52/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9567 - loss: 0.1289 - val_accuracy: 0.5832 - val_loss: 2.1929 - learning_rate: 5.0000e-05\n",
      "Epoch 53/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9578 - loss: 0.1269 - val_accuracy: 0.5810 - val_loss: 2.1604 - learning_rate: 5.0000e-05\n",
      "Epoch 54/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9614 - loss: 0.1239 - val_accuracy: 0.5724 - val_loss: 2.2021 - learning_rate: 5.0000e-05\n",
      "Epoch 55/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9723 - loss: 0.1087 - val_accuracy: 0.5745 - val_loss: 2.1728 - learning_rate: 2.5000e-05\n",
      "Epoch 56/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9605 - loss: 0.1115 - val_accuracy: 0.5788 - val_loss: 2.1537 - learning_rate: 2.5000e-05\n",
      "Epoch 57/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9680 - loss: 0.1047 - val_accuracy: 0.5680 - val_loss: 2.1719 - learning_rate: 2.5000e-05\n",
      "Epoch 58/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 50ms/step - accuracy: 0.9686 - loss: 0.1076 - val_accuracy: 0.5832 - val_loss: 2.1592 - learning_rate: 2.5000e-05\n",
      "Epoch 59/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9674 - loss: 0.1117 - val_accuracy: 0.5767 - val_loss: 2.1670 - learning_rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 04:42:39.406172: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-11-04 04:42:40.501549: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 380 bytes spill stores, 244 bytes spill loads\n",
      "\n",
      "2024-11-04 04:42:40.784712: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2024-11-04 04:42:41.684310: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2024-11-04 04:42:42.373165: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9', 576 bytes spill stores, 392 bytes spill loads\n",
      "\n",
      "2024-11-04 04:42:42.404467: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 1444 bytes spill stores, 1432 bytes spill loads\n",
      "\n",
      "2024-11-04 04:42:42.739238: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-04 04:42:43.047780: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.5687 - loss: 2.0706\n",
      "Validation Loss: 2.0891897678375244, Validation Accuracy: 0.5788336992263794\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu,True)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "\n",
    "# Load the data (assuming your data files are in the correct directory as specified)\n",
    "directory = '/app/data/filteredData'\n",
    "balltoball = pl.read_csv(os.path.join(directory, 'balltoball.csv'))\n",
    "teamStats = pl.read_csv(os.path.join(directory, 'team12Stats.csv'))\n",
    "playersStats = pl.read_csv(os.path.join(directory, 'playersStats.csv'))\n",
    "\n",
    "\n",
    "partitions = teamStats.partition_by(['match_id', 'flip'])\n",
    "partitions = np.array([partition.drop(['match_id','flip']).to_numpy() for partition in partitions])\n",
    "tstf = tf.data.Dataset.from_tensor_slices(partitions)\n",
    "partitions = playersStats.partition_by(['match_id', 'flip'])\n",
    "partitions = np.array([partition.drop(['match_id','flip']).to_numpy() for partition in partitions])\n",
    "pstf = tf.data.Dataset.from_tensor_slices(partitions)\n",
    "partitions = balltoball.partition_by(['match_id', 'flip'])\n",
    "# Create a ragged tensor from a list of tensors\n",
    "ragged_tensor = tf.ragged.constant([partition.drop(['match_id','flip']).to_numpy() for partition in partitions])\n",
    "bbtf = tf.data.Dataset.from_tensor_slices(ragged_tensor)\n",
    "\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "for sample in tstf.take(1):\n",
    "    print(\"Team Stats Sample Shape:\", sample.shape)\n",
    "\n",
    "for sample in pstf.take(1):\n",
    "    print(\"Players Stats Sample Shape:\", sample.shape)\n",
    "\n",
    "for sample in bbtf.take(1):\n",
    "    print(\"Ball to Ball Stats Sample Shape:\", sample.shape)\n",
    "    print(\"Ball to Ball Stats Sample Value:\", sample[0])\n",
    "\n",
    "combined_dataset = tf.data.Dataset.zip((tstf, pstf, bbtf))\n",
    "for sample in combined_dataset.take(1):\n",
    "    print(\"Team Stats Sample Shape:\", sample[0].shape)\n",
    "    print(\"Players Stats Sample Shape:\", sample[1].shape)\n",
    "    print(\"Ball to Ball Stats Sample Shape:\", sample[2].shape)\n",
    "    print(\"Sample 0:\", sample)\n",
    "\n",
    "\n",
    "# Assuming `combined_dataset` is your tf.data.Dataset containing the ball stats and labels\n",
    "def extract_labels_and_data(combined_dataset):\n",
    "    data_samples = []\n",
    "    labels = []\n",
    "    for sample in combined_dataset:\n",
    "        # Convert ragged tensor to uniform tensor\n",
    "        ball_stats = sample[2].to_tensor()\n",
    "        # Extract ball stats and labels\n",
    "        data_sample = ball_stats[:, :-1]  # Assuming last column is the label\n",
    "        label = ball_stats[:, -1]  # Last column as labels\n",
    "        data_samples.append(data_sample)\n",
    "        labels.append(label)\n",
    "    return data_samples, labels\n",
    "\n",
    "ball_data_samples, labels = extract_labels_and_data(combined_dataset)\n",
    "# Prepare the data for training\n",
    "def prepare_dataset(combined_dataset):\n",
    "    team_stats_data = []\n",
    "    player_stats_data = []\n",
    "    ball_stats_data = []\n",
    "    labels = []\n",
    "    for sample in combined_dataset:\n",
    "        team_stats_sample = sample[0]\n",
    "        player_stats_sample = sample[1]\n",
    "        ball_stats_sample = sample[2].to_tensor()\n",
    "\n",
    "        # Assuming last column is the label\n",
    "        data_sample = ball_stats_sample[:, :-1]\n",
    "        label = ball_stats_sample[0, -1]  # Assuming label is the same across the sequence\n",
    "\n",
    "        team_stats_data.append(team_stats_sample)\n",
    "        player_stats_data.append(player_stats_sample)\n",
    "        ball_stats_data.append(data_sample)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Pad ball_stats_data sequences to the same length\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    ball_stats_data = pad_sequences([data.numpy() for data in ball_stats_data], padding='post', dtype='float32')\n",
    "\n",
    "    return (tf.stack(team_stats_data), tf.stack(player_stats_data), tf.convert_to_tensor(ball_stats_data)), tf.convert_to_tensor(labels)\n",
    "\n",
    "# Prepare the dataset\n",
    "inputs, labels = prepare_dataset(combined_dataset)\n",
    "\n",
    "# Adjust input shapes based on prepared data\n",
    "team_input_shape = inputs[0].shape[1:]\n",
    "player_input_shape = inputs[1].shape[1:]\n",
    "team_input_shape, player_input_shape, inputs[2].shape, labels.shape\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu,True)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "\n",
    "# Load the data (assuming your data files are in the correct directory as specified)\n",
    "directory = '/app/data/filteredData'\n",
    "balltoball = pl.read_csv(os.path.join(directory, 'balltoball.csv'))\n",
    "teamStats = pl.read_csv(os.path.join(directory, 'team12Stats.csv'))\n",
    "playersStats = pl.read_csv(os.path.join(directory, 'playersStats.csv'))\n",
    "\n",
    "\n",
    "partitions = teamStats.partition_by(['match_id', 'flip'])\n",
    "partitions = np.array([partition.drop(['match_id','flip']).to_numpy() for partition in partitions])\n",
    "tstf = tf.data.Dataset.from_tensor_slices(partitions)\n",
    "partitions = playersStats.partition_by(['match_id', 'flip'])\n",
    "partitions = np.array([partition.drop(['match_id','flip']).to_numpy() for partition in partitions])\n",
    "pstf = tf.data.Dataset.from_tensor_slices(partitions)\n",
    "partitions = balltoball.partition_by(['match_id', 'flip'])\n",
    "# Create a ragged tensor from a list of tensors\n",
    "ragged_tensor = tf.ragged.constant([partition.drop(['match_id','flip']).to_numpy() for partition in partitions])\n",
    "bbtf = tf.data.Dataset.from_tensor_slices(ragged_tensor)\n",
    "\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "for sample in tstf.take(1):\n",
    "    print(\"Team Stats Sample Shape:\", sample.shape)\n",
    "\n",
    "for sample in pstf.take(1):\n",
    "    print(\"Players Stats Sample Shape:\", sample.shape)\n",
    "\n",
    "for sample in bbtf.take(1):\n",
    "    print(\"Ball to Ball Stats Sample Shape:\", sample.shape)\n",
    "    print(\"Ball to Ball Stats Sample Value:\", sample[0])\n",
    "\n",
    "combined_dataset = tf.data.Dataset.zip((tstf, pstf, bbtf))\n",
    "for sample in combined_dataset.take(1):\n",
    "    print(\"Team Stats Sample Shape:\", sample[0].shape)\n",
    "    print(\"Players Stats Sample Shape:\", sample[1].shape)\n",
    "    print(\"Ball to Ball Stats Sample Shape:\", sample[2].shape)\n",
    "    print(\"Sample 0:\", sample)\n",
    "\n",
    "# Assuming `combined_dataset` is your tf.data.Dataset containing the ball stats and labels\n",
    "def extract_labels_and_data(combined_dataset):\n",
    "    data_samples = []\n",
    "    labels = []\n",
    "    for sample in combined_dataset:\n",
    "        # Convert ragged tensor to uniform tensor\n",
    "        ball_stats = sample[2].to_tensor()\n",
    "        # Extract ball stats and labels\n",
    "        data_sample = ball_stats[:, :-1]  # Assuming last column is the label\n",
    "        label = ball_stats[:, -1]  # Last column as labels\n",
    "        data_samples.append(data_sample)\n",
    "        labels.append(label)\n",
    "    return data_samples, labels\n",
    "\n",
    "ball_data_samples, labels = extract_labels_and_data(combined_dataset)\n",
    "# Prepare the data for training\n",
    "def prepare_dataset(combined_dataset):\n",
    "    team_stats_data = []\n",
    "    player_stats_data = []\n",
    "    ball_stats_data = []\n",
    "    labels = []\n",
    "    for sample in combined_dataset:\n",
    "        team_stats_sample = sample[0]\n",
    "        player_stats_sample = sample[1]\n",
    "        ball_stats_sample = sample[2].to_tensor()\n",
    "\n",
    "        # Assuming last column is the label\n",
    "        data_sample = ball_stats_sample[:, :-1]\n",
    "        label = ball_stats_sample[0, -1]  # Assuming label is the same across the sequence\n",
    "\n",
    "        team_stats_data.append(team_stats_sample)\n",
    "        player_stats_data.append(player_stats_sample)\n",
    "        ball_stats_data.append(data_sample)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Pad ball_stats_data sequences to the same length\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    ball_stats_data = pad_sequences([data.numpy() for data in ball_stats_data], padding='post', dtype='float32')\n",
    "\n",
    "    return (tf.stack(team_stats_data), tf.stack(player_stats_data), tf.convert_to_tensor(ball_stats_data)), tf.convert_to_tensor(labels)\n",
    "\n",
    "# Prepare the dataset\n",
    "inputs, labels = prepare_dataset(combined_dataset)\n",
    "\n",
    "# Adjust input shapes based on prepared data\n",
    "team_input_shape = inputs[0].shape[1:]\n",
    "player_input_shape = inputs[1].shape[1:]\n",
    "print(team_input_shape, player_input_shape, inputs[2].shape, labels.shape)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import layers\n",
    "# Define the Team Stats Model (DNN)\n",
    "class TeamStatsModel(tf.keras.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(TeamStatsModel, self).__init__()\n",
    "        self.dense1 = layers.Dense(64, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.dropout1 = layers.Dropout(0.3)\n",
    "        \n",
    "        self.dense2 = layers.Dense(32, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.dropout2 = layers.Dropout(0.3)\n",
    "        \n",
    "        self.output_layer = layers.Dense(16, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Define the Player Stats Model (CNN)\n",
    "class PlayerStatsModel(tf.keras.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(PlayerStatsModel, self).__init__()\n",
    "        self.conv1 = layers.Conv1D(32, kernel_size=3, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.pool1 = layers.MaxPooling1D(pool_size=2)\n",
    "\n",
    "        self.conv2 = layers.Conv1D(64, kernel_size=3, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.pool2 = layers.MaxPooling1D(pool_size=2)\n",
    "\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.output_layer = layers.Dense(16, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)  # Fixed from inputs to x\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Update the BallToBallModel\n",
    "class BallToBallModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(BallToBallModel, self).__init__()\n",
    "        # Add a projection layer to match the input dimension to the model dimension\n",
    "        self.projection = layers.Dense(128)\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.attention1 = layers.MultiHeadAttention(num_heads=8, key_dim=128)\n",
    "        self.dropout1 = layers.Dropout(0.3)\n",
    "        self.ffn1 = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "            layers.Dense(128, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "        ])\n",
    "\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.attention2 = layers.MultiHeadAttention(num_heads=8, key_dim=128)\n",
    "        self.dropout2 = layers.Dropout(0.3)\n",
    "        self.ffn2 = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "            layers.Dense(128, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "        ])\n",
    "\n",
    "        self.layer_norm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.attention3 = layers.MultiHeadAttention(num_heads=8, key_dim=128)\n",
    "        self.dropout3 = layers.Dropout(0.3)\n",
    "        self.ffn3 = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "            layers.Dense(128, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "        ])\n",
    "\n",
    "        self.global_pool = layers.GlobalAveragePooling1D()\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Dense(64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Dense(16, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Project inputs to match the model dimension\n",
    "        x = self.projection(inputs)\n",
    "        x = self.layer_norm1(x)\n",
    "        attn_output = self.attention1(x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        x = x + attn_output  # Residual connection\n",
    "\n",
    "        ffn_output = self.ffn1(x)\n",
    "        x = x + ffn_output  # Residual connection\n",
    "\n",
    "        # Second transformer block\n",
    "        x = self.layer_norm2(x)\n",
    "        attn_output = self.attention2(x, x)\n",
    "        attn_output = self.dropout2(attn_output)\n",
    "        x = x + attn_output\n",
    "\n",
    "        ffn_output = self.ffn2(x)\n",
    "        x = x + ffn_output\n",
    "\n",
    "        # Third transformer block\n",
    "        x = self.layer_norm3(x)\n",
    "        attn_output = self.attention3(x, x)\n",
    "        attn_output = self.dropout3(attn_output)\n",
    "        x = x + attn_output\n",
    "\n",
    "        ffn_output = self.ffn3(x)\n",
    "        x = x + ffn_output\n",
    "\n",
    "        # Global pooling and final MLP\n",
    "        x = self.global_pool(x)\n",
    "        return self.mlp(x)\n",
    "\n",
    "# Update the CombinedModel to reflect the change\n",
    "class CombinedModel(tf.keras.Model):\n",
    "    def __init__(self, team_input_shape, player_input_shape):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.team_model = TeamStatsModel(team_input_shape)\n",
    "        self.player_model = PlayerStatsModel(player_input_shape)\n",
    "        self.ball_model = BallToBallModel()\n",
    "        \n",
    "        self.final_mlp1 = layers.Dense(64, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "        self.final_mlp2 = layers.Dense(32, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.final_output = layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        team_input, player_input, ball_input = inputs\n",
    "        team_output = self.team_model(team_input)        # Shape: (batch_size, 1, 16)\n",
    "        team_output = layers.Flatten()(team_output)      # Shape: (batch_size, 16)\n",
    "        player_output = self.player_model(player_input)  # Shape: (batch_size, 16)\n",
    "        ball_output = self.ball_model(ball_input)        # Shape: (batch_size, 16)\n",
    "        \n",
    "        # Concatenate along the last axis\n",
    "        combined = layers.concatenate([team_output, player_output, ball_output], axis=-1)\n",
    "        x = self.final_mlp1(combined)\n",
    "        x = self.dropout(x)\n",
    "        x = self.final_mlp2(x)\n",
    "        return self.final_output(x)\n",
    "\n",
    "# Import callbacks for learning rate scheduling and early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check the shapes of inputs and labels\n",
    "team_input, player_input, ball_input = inputs\n",
    "print(f\"Shape of team_input: {team_input.shape}\")\n",
    "print(f\"Shape of player_input: {player_input.shape}\")\n",
    "print(f\"Shape of ball_input: {ball_input.shape}\")\n",
    "print(f\"Shape of labels: {labels.shape}\")\n",
    "\n",
    "# Ensure inputs and labels have the same number of samples\n",
    "min_samples = min(team_input.shape[0], player_input.shape[0], ball_input.shape[0], labels.shape[0])\n",
    "team_input = team_input[:min_samples]\n",
    "player_input = player_input[:min_samples]\n",
    "ball_input = ball_input[:min_samples]\n",
    "labels = labels[:min_samples]\n",
    "\n",
    "# Ensure inputs have the same shape\n",
    "team_input = tf.reshape(team_input, (min_samples, *team_input_shape))\n",
    "player_input = tf.reshape(player_input, (min_samples, *player_input_shape))\n",
    "ball_input = tf.reshape(ball_input, (min_samples, *inputs[2].shape[1:]))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "(train_team_input, val_team_input, train_player_input, val_player_input, train_ball_input, val_ball_input, train_labels, val_labels) = train_test_split(\n",
    "    team_input.numpy(), player_input.numpy(), ball_input.numpy(), labels.numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the split data back to tensors\n",
    "train_team_input = tf.convert_to_tensor(train_team_input)\n",
    "val_team_input = tf.convert_to_tensor(val_team_input)\n",
    "train_player_input = tf.convert_to_tensor(train_player_input)\n",
    "val_player_input = tf.convert_to_tensor(val_player_input)\n",
    "train_ball_input = tf.convert_to_tensor(train_ball_input)\n",
    "val_ball_input = tf.convert_to_tensor(val_ball_input)\n",
    "train_labels = tf.convert_to_tensor(train_labels)\n",
    "val_labels = tf.convert_to_tensor(val_labels)\n",
    "\n",
    "# Combine the inputs for training and validation\n",
    "train_inputs = [train_team_input, train_player_input, train_ball_input]\n",
    "val_inputs = [val_team_input, val_player_input, val_ball_input]\n",
    "\n",
    "# Instantiate and compile the model with a learning rate scheduler\n",
    "model = CombinedModel(team_input_shape, player_input_shape)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Define callbacks\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "# Add callbacks for early stopping and model checkpointing\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Train the model with updated callbacks\n",
    "model.fit(\n",
    "    train_inputs, train_labels,\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    validation_data=(val_inputs, val_labels),\n",
    "    callbacks=[early_stopping, lr_scheduler, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(val_inputs, val_labels)\n",
    "print(f\"Validation Loss: {loss}, Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'h'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'h'"
     ]
    }
   ],
   "source": [
    "model.history.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
